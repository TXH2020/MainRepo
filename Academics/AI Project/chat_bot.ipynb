{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TXH2020/MainRepo/blob/main/Academics/AI%20Project/chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu1oM4oibNPv",
        "outputId": "8eafaba1-fb2a-4313-f747-1514c7a1ec48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('intents.json', <http.client.HTTPMessage at 0x7ac834f126b0>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import urllib.request as ureq\n",
        "ureq.urlretrieve('https://raw.githubusercontent.com/TXH2020/fast-labeling-workflow/master/Datasets/webintents.json','intents.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVNYWiYYWv4Z",
        "outputId": "b00e4d87-94f0-4046-e8b0-99240a1875a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# things we need for NLP\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "nltk.download('punkt')\n",
        "# things we need for Tensorflow\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XDtfKI_5Wv4h"
      },
      "outputs": [],
      "source": [
        "# import our chat-bot intents file\n",
        "import json\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xl0bb-YNWv4i"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "# loop through each sentence in our intents patterns\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in our corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "# stem and lower each word and remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicates\n",
        "classes = sorted(list(set(classes)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y4e7Y6pVWv4k"
      },
      "outputs": [],
      "source": [
        "# create our training data\n",
        "training = []\n",
        "output = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training,dtype='object')\n",
        "\n",
        "# create train and test lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmmQjaH_Wv4m",
        "outputId": "c48b2005-a7c7-4264-842f-806a5ac149bb",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ac7aaeee890>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# reset underlying graph data\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "model=keras.Sequential([\n",
        "    keras.layers.Dense(8,input_shape=(len(train_x[0]),),kernel_initializer='normal'),\n",
        "    keras.layers.Dense(8,kernel_initializer='normal'),\n",
        "    keras.layers.Dense(len(train_y[0]),activation='softmax',kernel_initializer='normal')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mean_squared_logarithmic_error', metrics=['accuracy'])\n",
        "model.fit(np.array(train_x),np.array(train_y), epochs=1000, batch_size=8, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "nttX7mKZWv4o"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "\n",
        "    return(np.array(bag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqbbBLwxWv4p",
        "outputId": "b665558d-0404-4756-d78b-9e3607369f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "['1st&2ndsem', '1styearsyll', '2ndyearsyll', '3rdsem', '3rdyearsyll', '4thsem', '4thyearsyll', '5thsem', '6thsem', '7thsem', '8thsem', 'ADD', 'AI', 'AP Maam', 'Akshata Maam', 'Bhushan Sir', 'BigData', 'CD', 'CNL', 'CNS', 'COA', 'Chandrika Maam', 'Chetan Sir', 'Cloud', 'DAA', 'DAALAB', 'DBMS', 'DCN', 'DL', 'DMML', 'DMS', 'DRB Sir', 'DS', 'Darshana Maam', 'Faculty Profiles', 'FinalProject', 'Ganesh Sir', 'Geeta Maam', 'HOD', 'HPC', 'IOT', 'Jamuna Maam', 'Java', 'Jayalakshmi Maam', 'Kamath Maam', 'MAD', 'MP', 'Mallegowda Sir', 'Mamatha Maam', 'Manjula Maam', 'MiniProject', 'Monica Maam', 'Nandini Maam', 'OOPS', 'OS', 'PAP', 'PMEE', 'Pallavi Maam', 'Papers', 'Parkavi Maam', 'Pradeep Sir', 'Python', 'RPA', 'SE', 'SRR Maam', 'Sangeetha J Maam', 'Sangeetha V Maam', 'Shilpa C Maam', 'Software Defined Networks', 'Soumya Maam', 'Sushma Maam', 'TNR Sir', 'TOC', 'USP', 'Veena Maam', 'Viswachetan Sir', 'Wireless', 'courses', 'exam', 'pgcnesyll20-22', 'pgcsesyll20-22', 'pgcsesyll21-23']\n"
          ]
        }
      ],
      "source": [
        "p = bow(\"1st sem courses\", words)\n",
        "print (p)\n",
        "print (classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_XlaL6GWv4s",
        "outputId": "db643070-412c-45d3-ba26-35dbd2b88e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 95ms/step\n",
            "[[9.99527693e-01 1.57815867e-20 7.51073627e-18 4.65831889e-08\n",
            "  1.95703793e-32 1.21517326e-06 0.00000000e+00 1.84546261e-05\n",
            "  1.37313342e-04 5.86906843e-11 3.15237965e-04 8.10181931e-35\n",
            "  1.32319527e-29 6.87635102e-33 2.44169109e-27 1.29578008e-17\n",
            "  3.99016831e-19 0.00000000e+00 4.25608350e-23 5.67899406e-13\n",
            "  1.20376863e-35 5.95503327e-24 5.14170920e-14 4.89284278e-29\n",
            "  0.00000000e+00 0.00000000e+00 2.00846133e-38 4.99488344e-08\n",
            "  1.00508853e-27 0.00000000e+00 4.57239965e-17 2.71242552e-19\n",
            "  3.27920012e-23 6.02250054e-37 2.32617947e-23 0.00000000e+00\n",
            "  3.93265558e-26 3.16065919e-22 1.09494895e-35 5.60384738e-24\n",
            "  8.36359629e-19 1.84329988e-23 4.42442225e-33 1.34555649e-21\n",
            "  1.88832747e-26 3.33088049e-29 2.52070646e-28 9.63157732e-20\n",
            "  5.10698016e-30 7.84460639e-23 0.00000000e+00 5.29258812e-35\n",
            "  2.79069381e-25 4.85116138e-27 7.52138958e-29 7.31770890e-35\n",
            "  9.16289949e-34 2.92985814e-33 4.80388678e-13 3.82835530e-36\n",
            "  0.00000000e+00 2.65376075e-26 0.00000000e+00 1.19029596e-16\n",
            "  1.00708351e-21 1.45607093e-28 3.98496783e-18 1.46809571e-28\n",
            "  1.09056962e-13 1.29583375e-18 1.17298510e-12 2.38124098e-25\n",
            "  2.39289950e-30 5.55159261e-12 2.62873615e-31 7.84296482e-37\n",
            "  8.21556575e-29 3.20634292e-08 4.83739565e-19 0.00000000e+00\n",
            "  4.11409798e-34 0.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "print(model.predict(np.array([p])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('m.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "v25AuqCbW_Vp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('i.pkl','wb') as f:\n",
        "  pickle.dump(intents,f)\n",
        "with open('c.pkl','wb') as f:\n",
        "  pickle.dump(classes,f)\n",
        "with open('w.pkl','wb') as f:\n",
        "  pickle.dump(words,f)"
      ],
      "metadata": {
        "id": "1pRJXBdBYC_7"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}